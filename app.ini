[uwsgi]
module = llm_service:app
master = true
processes = 1
http = :5002
chmod-socket = 666
vacuum = true
die-on-term = true

# 显式指定 Python 解释器路径
python-exec = /usr/local/bin/python3

# 增加 Python 模块搜索路径，确保能找到项目和依赖
pythonpath = /ai/mllm/
pythonpath = /usr/local/lib/python3.12/dist-packages/

# 显式设置环境变量，以规避 libucc.so.1 等问题
env = LD_PRELOAD=
env = LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:
env = MODEL_PATH=/ai/mllm/models/Qwen3-0.6B/ # 使用绝对路径设置模型路径 (保持存在)

single-interpreter = true
logto = uwsgi.log

# ====== 新增或修改部分 ======
enable-threads = true # 显式启用线程支持
catch-exceptions = true # 尝试捕获所有 Python 异常
py-call-osafterfork = true # 处理 Python os.fork_exec cleanup
thunder-lock = true # 显式启用，解决多进程启动时的竞争条件

# ====== 新增内存限制 ======
limit-as = 32768 # 限制虚拟内存为 32GB (大幅增加)
max-rss = 24576 # 限制物理内存为 24GB (大幅增加)

# ====== 移除或修改部分 ======
# 移除非标准或可能冲突的参数
# cuda-hooks = true # 移除此行
# close-on-exec = true # 移除此行
# master-fifo = 666 # 移除此行

# 重新确认惰性加载，即使代码已改用每个请求加载
lazy-apps = true # 确保应用在 worker 进程中独立加载
